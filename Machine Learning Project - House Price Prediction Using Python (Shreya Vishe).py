#!/usr/bin/env python
# coding: utf-8

# ## MACHINE LEARNING PROJECT

# ## Project Name: House Price Prediction using Machine Learning in Python
# 
# ## By- Shreya Vishe
# 
# ## Under the guidance of: Mr. Sameer Warsolkar

# ## Introduction 
# 
# Thousands of houses are sold everyday. There are some questions every buyer asks himself like: What is the actual price that this house deserves? Am I paying a fair price? In this poject, a machine learning model is proposed to predict a house price based on data related to the house and its location. During the development and evaluation of our model, we will show the code used for each step followed by its output. This will facilitate the reproducibility of our work. In this study, Python programming language with a number of Python packages will be used.

# ## Objective
# 
# Predict the selling price of houses based on various features using a machine learning model.

# ## The dataset contains 7 features
# 
#     
# Avg. Area Income: The average income of residents in a specific area.
# 
# Avg. Area House Age: The average age of houses in a particular area.
# 
# Avg. Area Number of Rooms: The average number of rooms in houses in the area.
# 
# Avg. Area Number of Bedrooms: The average number of bedrooms in houses in the area.
# 
# Area Population: The population of the area.
# 
# Price: The target variable to be predicted, likely representing the price of houses in the area.
# 
# Address: The address of the houses in the dataset.

# ### Importing Libraries and Dataset

# ###  Importing libraries provides ready-made tools for data manipulation, analysis, and visualization, while importing datasets allows you to explore, clean, and analyze data in your programming environment. It's a fundamental step for efficient and effective data analysis and machine learning.

# In[1]:


#Importing Libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings("ignore")


# ### Here we are using
# 
# Numpy: For efficient data representation, manipulation, and mathematical operations. 
# 
# Pandas: To load the data.
# 
# Matplotlib: To visualize the data features.
# 
# Seaborn: To see the correlation between features using heatmap.
# 
# Warnings: To suppress all warnings generated by program code.

# ### Reading the Data 

# In[2]:


df = pd.read_csv("USA_Housing.csv")
df


# In[3]:


#To get the first five rows from the dataset

df.head()


# In[4]:


#To get the last five rows from the dataset

df.tail()


# In[5]:


#Data Preprocessing (To categorize the features depending on their datatype (int, float, object) and then calculate the number of them.)

categorical_cols = list(df.select_dtypes(include=['object']).columns)
print("Categorical variables:", len(categorical_cols))
 
int_cols = list(df.select_dtypes(include=['int64']).columns)
print("Integer variables:", len(int_cols))
 
fl_cols = list(df.select_dtypes(include = ['float']).columns)
print("Float variables:",len(fl_cols))


# ###  Here, we can see the dataset is having 1 categorical variable, no integer, and 6 float.

# # EDA - Exploratory Data Analysis 

# ### Exploratory Data Analysis (EDA) is a crucial step in data analysis that involves exploring and visualizing a dataset to understand its main characteristics, patterns, and trends. EDA helps in making informed decisions about data cleaning, feature engineering, and the direction of further analysis.

# In[6]:


#To get the information about the data

df.info()


# ### Here, we can see there are 7 attributes in total out of which 64 are float and 1 object.

# In[7]:


#Handeling missing values

df.isnull().sum()


# ### As we can see, there are no null values present in this dataset.

# In[8]:


#To obtain the shape of a dataframe

df.shape


# ### There are 7 columns and 5000 rows in this dataset.

# In[9]:


#To display the statistical information of the data

df.describe()


# In[10]:


# To get the information about the data

df.info()


# ### Here, we can see there are 7 attributes in total out of which 64 are float and 1 object.

# In[11]:


#Handeling missing values

df.isnull().sum()


# ### As we can see here, there are no missing values in this dataset.

# In[12]:


#To get the information about the data

df.info()


# ### Here, we can see there are 7 attributes in total out of which 64 are float and 1 object.

# In[13]:


#To compute the pairwise correlation of columns in a DataFrame

df.corr()


# ### Lable Encoding

# ### A method in machine learning to convert categorical data into numerical form by assigning a unique integer to each category. It's useful for preprocessing data before feeding it into algorithms that require numerical input. 

# In[14]:


#Importing Lable Encoder 

from sklearn.preprocessing import LabelEncoder, MinMaxScaler


# ###  Here we are using
# 
# Sklearn:Python library for machine learning, offering a versatile set of tools and consistent interfaces for building and evaluating machine learning models. 
# 
# Sklearn.preprocessing: A module in scikit-learn that offers tools for preparing data before using it in machine learning models. It includes functions for scaling, encoding categorical variables, handling missing data, and more. 
# 
# Lable Encoder: To normalize labels.
# 
# Min Max Scaler:A normalization technique used in machine learning to scale and transform numerical features within a specific range, usually between 0 and 1.

# In[15]:


#Selecting the columns in the DataFrame with data type object

catdata=df.select_dtypes(object)


# In[16]:


#To get information about the categorical features present in the dataset

cat = df.select_dtypes(object).columns
object_cols = list(cat)
print("Categorical variables:",object_cols)
print('No. of categorical features: ', len(object_cols))


# ### Here we have only  categorical variables i.e. 'Address'

# In[17]:


#To display the names of the categorical columns in your DataFrame

object_cols


# In[18]:


#Assuming 'object_cols' contains the names of categorical columns

le = LabelEncoder()
for i in object_cols:
    df[i]=le.fit_transform(df[i])
le


# ### The categorical variables in a DataFrame are converted into numerical labels

# In[19]:


#To get the information about the data

df.info()


# ### After converting the categorical column into numerical we have 6 float and 1 integer

# In[20]:


#Splitting the data into X and Y

from sklearn.model_selection import train_test_split


# ###  Here we are using
# 
# sklearn.model_selection:Essential function for assessing and improving the performance of machine learning models.
# 
# train_test_split: To split a dataset into training and testing sets, facilitating the evaluation of machine learning models.

# In[21]:


#Features (independent variables)

X = df.drop(['Price'], axis=1)
X


# In[22]:


#Target variable (dependent variable)

Y = df['Price']
Y


# In[23]:


# Split the training set into training and testing set

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)
print(X_train, X_test)
print(Y_train, Y_test)


# ### Splitting the training set into training and testing sets is essential for evaluating how well a machine learning model performs on new, unseen data. It helps prevent overfitting, allows for model evaluation, and supports hyperparameter tuning.

# ### Data Visualization

# ### Essential because it transforms raw data into a format that is accessible, understandable, and actionable. It plays a crucial role in the data analysis process, helping individuals at various levels of expertise derive insights, make informed decisions, and communicate findings effectively.

# In[24]:


plt.figure(figsize=(12, 6))
sns.heatmap(df.corr(),
            cmap='coolwarm',
            annot=True,
            fmt='.2f',
            linewidths=2)
plt.title('Correlation Matrix Heatmap')
plt.show()


# ### Here we have used Heat maps, primarily to visually represent patterns, relationships, and variations in data.

# In[28]:


data_num=df.columns
plt.figure(figsize=(12,20))

for i, col in enumerate(data_num):
    
    plt.subplot((len(data_num) + 1) // 2, 2, i+1 )
    sns.histplot(x=col,data=df,kde=True)    
plt.tight_layout(pad = 2)


# ### We have used Histograms as it helps in the data exploration and analysis phase.

# In[27]:


plt.figure(figsize=(18, 36))
plt.title('Categorical Features: Distribution')
plt.xticks(rotation=90)
index = 1

for index, col in enumerate(categorical_cols, start=1):
    plt.subplot(11, 4, index)
    plt.xticks(rotation=45, ha='right')  
    sns.countplot(x=col, data=df, palette='viridis')  

plt.tight_layout()  
plt.show()


# ###  We have used Subplots for visualizing multiple aspects of the data or model performance simultaneously. 

# ### Data Scaling

# ###  Data scaling is crucial in machine learning to ensure fair contributions from all features, speed up training, and improve model performance by reducing sensitivity to initial conditions and facilitating optimization algorithms.

# In[29]:


#Data normalization with sklearn

from sklearn.preprocessing import MinMaxScaler


# ### Normalizing data is a common preprocessing step in machine learning, and it refers to scaling the features to a standard range.

# In[30]:


#Fit scaler on training data

minmax = MinMaxScaler()
X_train = minmax.fit_transform(X_train)
X_test = minmax.transform(X_test)


# ### To insure that the training (X_train) and testing (X_test) data using the same MinMaxScaler instance for consistency.

# In[31]:


#Libraries Imported

from sklearn.metrics import mean_absolute_percentage_error, r2_score


# ###  Here we are using
# 
# sklearn.metrics:Provides a wide range of tools for evaluating the performance of machine learning models.
# 
# mean_absolute_percentage_error: Measures the mean absolute percentage difference between true and predicted values essential for assessing accuracy, especially in forecasting. 
# 
# r2_score:Computes the coefficient of determination (R-squared) to evaluate how well predictions approximate true values ued for measuring the proportion of variance explained.

# # Linear Regression

# ### Linear Regression is a statistical method that models the relationship between a dependent variable and one or more independent variables by fitting a linear equation. It aims to find the best-fit line that minimizes the sum of squared differences between predicted and actual values. 

# In[32]:


#Libraries Imported

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error


# ### Here we are using
# 
# mean_squared_error: Measures the average squared difference between predicted and actual values.

# In[33]:


#Assuming X, Y are your features and target

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)


# ### Dividing the dataset (X for features, Y for target) into training (X_train, Y_train) and testing (X_test, Y_test) sets. Before creating a linear regreesion model. 

# In[34]:


#Create a linear regression model

linear_model = LinearRegression()
linear_model.fit(X_train, Y_train)
linear_model


# In[35]:


#Making predictions 

Y_pred = linear_model.predict(X_test)
Y_pred


# In[36]:


#Finding accuraccy

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

mse = mean_squared_error(Y_test, Y_pred)
mae = mean_absolute_error(Y_test, Y_pred)
r2 = r2_score(Y_test, Y_pred)

print(f'Mean Squared Error: {mse}')
print(f'Mean Absolute Error: {mae}')
print(f'Accuracy Score: {r2}')


# # Random Forest

# ### Random Forest is an ensemble of decision trees, combining their predictions for improved accuracy and robustness. It is widely used for both classification and regression tasks.

# In[43]:


#Libraries Imported

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error


# ### Here we are using
# 
# sklearn.ensembl:These models build multiple decision trees to enhance predictive performance.
# 
# RandomForestRegressor: Constructs an ensemble of decision trees to predict continuous target variables.

# In[44]:


#Assuming X_train and Y_train are your feature and target variables

rf_regressor = RandomForestRegressor()
rf_regressor.fit(X_train, Y_train)
Y_pred=rf_regressor.predict(X_test)


# ### Here, we are assuming X_train and Y_train represent our feature and target variables, and fitting in a RandomForestRegressor model to the training data and making predictions on the test data.

# In[45]:


#Libraries Imported

from sklearn import metrics
from sklearn.metrics import mean_squared_error


# In[46]:


#Finding accuraccy

mse = mean_squared_error(Y_test, Y_pred)
mae = mean_absolute_error(Y_test, Y_pred)
r2 = r2_score(Y_test, Y_pred)

print(f'Mean Squared Error: {mse}')
print(f'Mean Absolute Error: {mae}')
print(f'Accuracy Score: {r2}')


# ### Conclusion:
# 
# The RandomForest algorithm gives us maximum Accuracy score is 0.8807123071963929. compared to the other machine learning classification algorithm.
# 
# The best accuracy with an accuracy score of 88%.
